{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Voice\n",
    "##### 500 hours of speech recordings, with speaker demographics\n",
    "\n",
    "*  1. Purpose of this notebook is to explore demographics data and filter audio files \n",
    "*  2. Convert MP3  audio files to to wav \n",
    "\n",
    "##### Author - James, Aalok, Reshma\n",
    "\n",
    "\n",
    "The corpus is split into several parts for your convenience. \n",
    "The subsets with “valid” in their name are audio clips that have had at least 2 people listen to them, and the majority of those listeners say the audio matches the text. \n",
    "The subsets with “invalid” in their name are clips that have had at least 2 listeners, and the majority say the audio does not match the clip.\n",
    "All other clips, ie. those with fewer than 2 votes, or those that have equal valid and invalid votes, have “other” in their name.\n",
    "\n",
    "* The “valid” and “other” subsets are further divided into 3 groups:\n",
    "* dev - for development and experimentation\n",
    "* train - for use in speech recognition training\n",
    "* test - for testing word error rate\n",
    "\n",
    "Each row of a csv file represents a single audio clip, and contains the following information:\n",
    "\n",
    "* filename - relative path of the audio file\n",
    "* text - supposed transcription of the audio\n",
    "* up_votes - number of people who said audio matches the text\n",
    "* down_votes - number of people who said audio does not match text\n",
    "* age - age of the speaker, if the speaker reported it\n",
    "* gender - gender of the speaker, if the speaker reported it\n",
    "* accent - accent of the speaker, if the speaker reported it\n",
    "\n",
    "\n",
    "* Age details\n",
    "    * teens: '< 19'\n",
    "    * twenties: '19 - 29'\n",
    "    * thirties: '30 - 39'\n",
    "    * fourties: '40 - 49'\n",
    "    * fifties: '50 - 59'\n",
    "    * sixties: '60 - 69'\n",
    "    * seventies: '70 - 79'\n",
    "    * eighties: '80 - 89'\n",
    "    * nineties: '> 89'\n",
    "    \n",
    "    \n",
    "* Gender details\n",
    "    * male\n",
    "    * female\n",
    "    * other\n",
    "\n",
    "\n",
    "* Accent details\n",
    "    * us: 'United States English'\n",
    "    * australia: 'Australian English'\n",
    "    * england: 'England English'\n",
    "    * canada: 'Canadian English'\n",
    "    * philippines: 'Filipino'\n",
    "    * hongkong: 'Hong Kong English'\n",
    "    * indian: 'India and South Asia (India, Pakistan, Sri Lanka)'\n",
    "    * ireland: 'Irish English'\n",
    "    * malaysia: 'Malaysian English'\n",
    "    * newzealand: 'New Zealand English'\n",
    "    * scotland: 'Scottish English'\n",
    "    * singapore: 'Singaporean English'\n",
    "    * southatlandtic: 'South Atlantic (Falkland Islands, Saint Helena)'\n",
    "    * african: 'Southern African (South Africa, Zimbabwe, Namibia)'\n",
    "    * wales: 'Welsh English'\n",
    "    * bermuda: 'West Indies and Bermuda (Bahamas, Bermuda, Jamaica, Trinidad)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'playsound'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e826d3ba557f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplaysound\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHBox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVBox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'playsound'"
     ]
    }
   ],
   "source": [
    "import playsound\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import tensorflow.keras\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util functions\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from scipy.ndimage.morphology import binary_dilation\n",
    "from typing import Optional, Union\n",
    "import webrtcvad\n",
    "import struct\n",
    "from scipy import stats\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "from python_speech_features import fbank\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten,LeakyReLU, Input, SpatialDropout1D, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "date     = '1003'\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "# from constants import SAMPLE_RATE, NUM_FBANKS\n",
    "# from utils import find_files, ensures_dir\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n",
    "# This sets the granularity of the VAD. Should not need to be changed.\n",
    "vad_window_length = 30  # In milliseconds\n",
    "# Number of frames to average together when performing the moving average smoothing.\n",
    "# The larger this value, the larger the VAD variations must be to not get smoothed out. \n",
    "vad_moving_average_width = 8\n",
    "# Maximum number of consecutive silent frames a segment can have.\n",
    "vad_max_silence_length = 6\n",
    "\n",
    "## Audio volume normalization\n",
    "audio_norm_target_dBFS = -30\n",
    "int16_max              = (2 ** 15) - 1\n",
    "\n",
    "def trim_long_silences(wav):\n",
    "    \"\"\"\n",
    "    Ensures that segments without voice in the waveform remain no longer than a \n",
    "    threshold determined by the VAD parameters in params.py.\n",
    "    :param wav: the raw waveform as a numpy array of floats \n",
    "    :return: the same waveform with silences trimmed away (length <= original wav length)\n",
    "    \"\"\"\n",
    "    # Compute the voice detection window size\n",
    "    vad_window_length = 30 \n",
    "    sampling_rate     = 16000\n",
    "    samples_per_window = (vad_window_length * sampling_rate) // 1000\n",
    "    \n",
    "    # Trim the end of the audio to have a multiple of the window size\n",
    "    wav = wav[:len(wav) - (len(wav) % samples_per_window)]\n",
    "    \n",
    "    # Convert the float waveform to 16-bit mono PCM\n",
    "    pcm_wave = struct.pack(\"%dh\" % len(wav), *(np.round(wav * int16_max)).astype(np.int16))\n",
    "    \n",
    "    # Perform voice activation detection\n",
    "    voice_flags = []\n",
    "    vad = webrtcvad.Vad(mode=3)\n",
    "    for window_start in range(0, len(wav), samples_per_window):\n",
    "        window_end = window_start + samples_per_window\n",
    "        voice_flags.append(vad.is_speech(pcm_wave[window_start * 2:window_end * 2],\n",
    "                                         sample_rate=sampling_rate))\n",
    "    voice_flags = np.array(voice_flags)\n",
    "    \n",
    "    # Smooth the voice detection with a moving average\n",
    "    def moving_average(array, width):\n",
    "        array_padded = np.concatenate((np.zeros((width - 1) // 2), array, np.zeros(width // 2)))\n",
    "        ret = np.cumsum(array_padded, dtype=float)\n",
    "        ret[width:] = ret[width:] - ret[:-width]\n",
    "        return ret[width - 1:] / width\n",
    "    \n",
    "    audio_mask = moving_average(voice_flags, vad_moving_average_width)\n",
    "    audio_mask = np.round(audio_mask).astype(np.bool)\n",
    "    \n",
    "    # Dilate the voiced regions\n",
    "    audio_mask = binary_dilation(audio_mask, np.ones(vad_max_silence_length + 1))\n",
    "    audio_mask = np.repeat(audio_mask, samples_per_window)\n",
    "    \n",
    "    return wav[audio_mask == True]\n",
    "\n",
    "def normalize_frames(m,Scale=True):\n",
    "    if Scale:\n",
    "        return (m - np.mean(m, axis=0)) / (np.std(m, axis=0) + 2e-12)\n",
    "    else:\n",
    "        return (m - np.mean(m, axis=0))\n",
    "\n",
    "def get_wav(filename):\n",
    "    '''\n",
    "    Load wav file from disk\n",
    "    :param language_num (list): list of file names\n",
    "    :return (numpy array): wav files\n",
    "    '''\n",
    "    wav, sr = librosa.load(filename)\n",
    "    return(wav)\n",
    "\n",
    "def get_kaldi_features_pred32(wav, dummies_):\n",
    "    '''\n",
    "    Get Kaldi - Discrete FFT features\n",
    "    :param wav_: list of trimmed wav file\n",
    "    :param y   : Array of accents\n",
    "    :param filename: Array of filenames \n",
    "    :return (numpy array): array of (mfcc, filter_banks, delta_1, delta_2), accent array (utternace level), dict(filename,number of frames)\n",
    "    '''\n",
    "    n_mfcc   = 13\n",
    "    n_filt   = 32\n",
    "    if len(wav) > 0:\n",
    "        mfcc_                  = mfcc(wav, samplerate=16000, winlen=0.025, winstep=0.01, numcep=n_mfcc)\n",
    "        filter_banks, energies = fbank(wav, samplerate=16000, nfilt=n_filt)\n",
    "        filter_banks           = 20 * np.log10(np.maximum(filter_banks,1e-5))\n",
    "        delta_1                = delta(filter_banks, N=1)\n",
    "        delta_2                = delta(delta_1, N=1)\n",
    "\n",
    "        filter_banks = normalize_frames(filter_banks, Scale=True)\n",
    "        delta_1      = normalize_frames(delta_1, Scale=True)\n",
    "        delta_2      = normalize_frames(delta_2, Scale=True)\n",
    "        dummies      = np.array(list(itertools.repeat(list(dummies_), len(mfcc_))))\n",
    "        frames_features = np.hstack([mfcc_, filter_banks, delta_1, delta_2, dummies])\n",
    "        #print(len(frames_features))\n",
    "    return frames_features\n",
    "\n",
    "def get_kaldi_features_pred64(wav, dummies_):\n",
    "    '''\n",
    "    Get Kaldi - Discrete FFT features\n",
    "    :param wav_: list of trimmed wav file\n",
    "    :param y   : Array of accents\n",
    "    :param filename: Array of filenames \n",
    "    :return (numpy array): array of (mfcc, filter_banks, delta_1, delta_2), accent array (utternace level), dict(filename,number of frames)\n",
    "    '''\n",
    "    n_mfcc   = 13\n",
    "    n_filt   = 64\n",
    "    if len(wav) > 0:\n",
    "        mfcc_                  = mfcc(wav, samplerate=16000, winlen=0.025, winstep=0.01, numcep=n_mfcc)\n",
    "        filter_banks, energies = fbank(wav, samplerate=16000, nfilt=n_filt)\n",
    "        filter_banks           = 20 * np.log10(np.maximum(filter_banks,1e-5))\n",
    "        delta_1                = delta(filter_banks, N=1)\n",
    "        delta_2                = delta(delta_1, N=1)\n",
    "\n",
    "        filter_banks = normalize_frames(filter_banks, Scale=True)\n",
    "        delta_1      = normalize_frames(delta_1, Scale=True)\n",
    "        delta_2      = normalize_frames(delta_2, Scale=True)\n",
    "        dummies      = np.array(list(itertools.repeat(list(dummies_), len(mfcc_))))\n",
    "        frames_features = np.hstack([mfcc_, filter_banks, delta_1, delta_2, dummies])\n",
    "        #print(len(frames_features))\n",
    "    return frames_features\n",
    "\n",
    "def convo_f1_score(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_audio = widgets.FileUpload(\n",
    "    accept='.wav',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=False,  # True to accept multiple files upload else False\n",
    "    description='Select a file'\n",
    ")\n",
    "\n",
    "drop_gender = widgets.Dropdown(\n",
    "    options=['male', 'female'],\n",
    "    value='female',\n",
    "    description='Gender:',\n",
    "    disabled=False,\n",
    ")\n",
    "    \n",
    "drop_age = widgets.Dropdown(\n",
    "    options=['eighties', 'fifties','fourties', 'seventies', 'sixties', 'teens', 'thirties', 'twenties'],\n",
    "    value='teens',\n",
    "    description='Age:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "button_predict = widgets.Button(\n",
    "    description='Predict',\n",
    ")\n",
    "\n",
    "button_play = widgets.Button(\n",
    "    description='Play Audio',\n",
    ")\n",
    "\n",
    "######################################## Reshma CNN1D #######################################################\n",
    "def model32(test_trim, dummy_dict):\n",
    "    test_features_32   = get_kaldi_features_pred32(test_trim, dummy_dict.values())\n",
    "    dependencies       = {'convo_f1_score': convo_f1_score}\n",
    "    model32            = tensorflow.keras.models.load_model('C:/Users/wanyi/Desktop/Uchicago/Deep Learning/Group Project/Fixed_Window/Reshma_final_code/Demo/models/nn_conv1d_32.h5',custom_objects=dependencies)\n",
    "    xtt                = test_features_32.shape\n",
    "    test_X_32          = np.reshape(test_features_32, (xtt[0], xtt[1], 1))\n",
    "    y_pred_test_32     = model32.predict(test_X_32)\n",
    "    y_pred_test_cls_32 = np.argmax(y_pred_test_32, axis=1)\n",
    "#   print (len(y_pred_test))\n",
    "    print(\"I am working hard to get the prediction!!!\")\n",
    "    #predicted\n",
    "    all_classes    = ['US', 'Australia', 'England', 'Indian', 'Canada']\n",
    "    demo_pred_32   = collections.Counter(y_pred_test_cls_32)\n",
    "#   print ('iamdemo32:',y_pred_test_cls_32)\n",
    "    results_32     = {all_classes[k] : v for k, v in demo_pred_32.items()}\n",
    "    results_df_32  = pd.DataFrame(results_32.items(), columns=['Accents', 'Counts'])\n",
    "    top_accent_32  = sorted(results_32.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "    return results_df_32, top_accent_32\n",
    "\n",
    "######################################## Reshma BLSTM #######################################################\n",
    "def model32_blstm(test_trim, dummy_dict):\n",
    "    test_features_32   = get_kaldi_features_pred32(test_trim, dummy_dict.values())\n",
    "    dependencies       = {'convo_f1_score': convo_f1_score}\n",
    "    model32            = tensorflow.keras.models.load_model('C:/Users/wanyi/Desktop/Uchicago/Deep Learning/Group Project/Fixed_Window/Reshma_final_code/Demo/models/nn_blstm.h5',custom_objects=dependencies)\n",
    "    xtt                = test_features_32.shape\n",
    "    test_X_32          = np.reshape(test_features_32, (xtt[0], xtt[1], 1))\n",
    "    y_pred_test_32     = model32.predict(test_X_32)\n",
    "    y_pred_test_cls_32 = np.argmax(y_pred_test_32, axis=1)\n",
    "#   print (len(y_pred_test))\n",
    "    #predicted\n",
    "    all_classes    = ['US', 'Australia', 'England', 'Indian', 'Canada']\n",
    "    demo_pred_32   = collections.Counter(y_pred_test_cls_32)\n",
    "#   print ('iamdemo32:',y_pred_test_cls_32)\n",
    "    results_32     = {all_classes[k] : v for k, v in demo_pred_32.items()}\n",
    "    results_df_32  = pd.DataFrame(results_32.items(), columns=['Accents', 'Counts'])\n",
    "    top_accent_32  = sorted(results_32.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "    return results_df_32, top_accent_32\n",
    "\n",
    "\n",
    "########################################## Aalok ########################################################\n",
    "def model32_alk(test_trim, dummy_dict):\n",
    "    test_features_32   = get_kaldi_features_pred32(test_trim, dummy_dict.values())\n",
    "    dependencies       = {'convo_f1_score': convo_f1_score}\n",
    "    model32_alk        = keras.models.load_model('C:/Users/wanyi/Desktop/Uchicago/Deep Learning/Group Project/Fixed_Window/Reshma_final_code/Demo/models/aalok_32.h5',custom_objects=dependencies)\n",
    "    xtt                = test_features_32.shape\n",
    "    test_X_32          = np.reshape(test_features_32, (xtt[0], xtt[1], 1))\n",
    "    y_pred_test_32     = model32_alk.predict(test_X_32)\n",
    "    y_pred_test_cls_32 = np.argmax(y_pred_test_32, axis=1)\n",
    "#     print (len(y_pred_test))\n",
    "    print(\"Yes, I am still running!!!\")\n",
    "    #predicted\n",
    "    all_classes    = ['US', \"Non-US\"]\n",
    "    demo_pred_32   = collections.Counter(y_pred_test_cls_32)\n",
    "#     print ('iamdemo32:',y_pred_test_cls_32)\n",
    "    results_32     = {all_classes[k] : v for k, v in demo_pred_32.items()}\n",
    "    results_df_32  = pd.DataFrame(results_32.items(), columns=['Accents', 'Counts'])\n",
    "    top_accent_32  = sorted(results_32.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "    return results_df_32, top_accent_32\n",
    "\n",
    "########################################### James #############################################\n",
    "\n",
    "def model64(test_trim, dummy_dict):\n",
    "    test_features   = get_kaldi_features_pred64(test_trim, dummy_dict.values())\n",
    "    dependencies    = {'convo_f1_score': convo_f1_score}\n",
    "    model64         = tensorflow.keras.models.load_model('C:/Users/wanyi/Desktop/Uchicago/Deep Learning/Group Project/Fixed_Window/Reshma_final_code/Demo/models/nn_conv1d_64.h5',custom_objects=dependencies)\n",
    "    xtt             = test_features.shape\n",
    "    test_X          = np.reshape(test_features, (xtt[0], xtt[1], 1))\n",
    "    y_pred_test     = model64.predict(test_X)\n",
    "    y_pred_test_cls = np.argmax(y_pred_test, axis=1)\n",
    "#     print (y_pred_test_cls)\n",
    "    print(\"Almost there, be patient!!!\")\n",
    "    #predicted\n",
    "    all_classes = ['North America', 'Europe', 'Indian', 'Oceania']\n",
    "    demo_pred   = collections.Counter(y_pred_test_cls)\n",
    "    results = {all_classes[k] : v for k, v in demo_pred.items()}\n",
    "    results_df = pd.DataFrame(results.items(), columns=['Accents', 'Counts'])\n",
    "    top_accent  = sorted(results.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "    return results_df, top_accent\n",
    "    \n",
    "def my_pred(b=None):\n",
    "    dummy_dict      = {'female':0, 'male':0, 'eighties':0, 'fifties':0,'fourties':0, 'seventies':0, 'sixties':0, 'teens':0, 'thirties':0, 'twenties':0}\n",
    "    audio_file      = next(iter(upload_audio.value))\n",
    "    path            = 'C:/Users/wanyi/Desktop/Uchicago/Deep Learning/Group Project/Fixed_Window/Reshma_final_code/Demo/Examples/'\n",
    "    dummy_dict[drop_gender.value]  = 1\n",
    "    dummy_dict[drop_age.value]     = 1\n",
    "    #print (audio_file,drop_gender.value, drop_age.value)\n",
    "    test_wav        = get_wav(path+audio_file) #get wav file\n",
    "    test_trim       = trim_long_silences(test_wav) # trim silences\n",
    "    results_32, accent_32 = model32(test_trim, dummy_dict)\n",
    "    results_64, accent_64 = model64(test_trim, dummy_dict)\n",
    "    results_32_alk, accent_32_alk = model32_alk(test_trim, dummy_dict)\n",
    "    results_32_blstm, accent_32_blstm = model32_blstm(test_trim, dummy_dict)\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(18,4))\n",
    "    axs[0].bar(results_32[\"Accents\"].values,results_32[\"Counts\"].values)\n",
    "    axs[0].set_title('Top5Accent CNN Model is:'+ accent_32)\n",
    "    axs[1].bar(results_32_blstm[\"Accents\"].values,results_32_blstm[\"Counts\"].values, color = \"C3\")\n",
    "    axs[1].set_title('Top5Accent BLSTM Model is:'+ accent_32_blstm)\n",
    "    axs[2].bar(results_32_alk[\"Accents\"].values,results_32_alk[\"Counts\"].values, color = 'C1')\n",
    "    axs[2].set_title('US/Non-US Model CNN is:'+ accent_32_alk)\n",
    "    axs[3].bar(results_64[\"Accents\"].values,results_64[\"Counts\"].values,color='C2')\n",
    "    axs[3].set_title('Continent Model CNN is:'+ accent_64)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_audio.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@button_predict.on_click\n",
    "def pred_button_clicked(b):\n",
    "        my_pred()\n",
    "        \n",
    "def listen_to_speech(b=None):\n",
    "    audio_file = next(iter(upload_audio.value))\n",
    "    path = 'C:/Users/wanyi/Desktop/Uchicago/Deep Learning/Group Project/Fixed_Window/Reshma_final_code/Demo/Examples/'\n",
    "    playsound.playsound(path+audio_file, True)\n",
    "\n",
    "@button_play.on_click\n",
    "def play_my_audio(b):\n",
    "    listen_to_speech()\n",
    "    \n",
    "tab1 = HBox(children=[upload_audio, \n",
    "                      button_play, \n",
    "                      drop_gender,\n",
    "                      drop_age,\n",
    "                      button_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = widgets.Tab(children=[tab1])#, tab2\n",
    "tab.set_title(0, 'Load and Predict')\n",
    "tab.set_title(1, 'Record and Predict')\n",
    "VBox(children=[tab])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
